{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics skills, and programming skills are equally important for analytics.', 'Statistics skills, and              domain knowledge are important for analytics.', 'I like reading books and travelling.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Statistics skills, and programming skills are equally important for analytics. Statistics skills, and              domain knowledge are important for analytics. I like reading books and travelling.\"\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics', 'skills', ',', 'and', 'programming', 'skills', 'are', 'equally', 'important', 'for', 'analytics', '.', 'Statistics', 'skills', ',', 'and', 'domain', 'knowledge', 'are', 'important', 'for', 'analytics', '.', 'I', 'like', 'reading', 'books', 'and', 'travelling', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word tokenizing\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample English sentence\n"
     ]
    }
   ],
   "source": [
    "#Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "#Function to remove stopwords\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    return \" \".join(stopwords_removed)\n",
    "print(remove_stopwords(\"This is a sample        English sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample English sentence with punctuations\n"
     ]
    }
   ],
   "source": [
    "#Remove punctuations\n",
    "import string\n",
    "#Function to remove punctuations\n",
    "def remove_punctuations(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "    return \" \".join(punt_removed)\n",
    "print(remove_punctuations(\"This is a sample English      sentence, with punctuations!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample English sentence, with whitespace and numbers 1234!\n"
     ]
    }
   ],
   "source": [
    "#Remove whitespace\n",
    "#Function to remove whitespace\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "text = \"This is a sample English sentence, \\n    with whitespace\\n\\n\\n and\\n numbers 1234!\"\n",
    "print(remove_whitespace(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Porter Stemmer-------\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "#Simple Stemming\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "#Porter Stemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"Python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "print(\"\\n-------Porter Stemmer-------\")\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Lancaster Stemmer-------\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "#Lancaster Stemmer\n",
    "ls = LancasterStemmer()\n",
    "example_words = [\"Python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "print(\"\\n-------Lancaster Stemmer-------\")\n",
    "for w in example_words:\n",
    "    print(ls.stem(w)) #'pythonly' is also converted to 'python' here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "child\n",
      "play\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizer\n",
    "#Simple Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('cats'))\n",
    "print(lemmatizer.lemmatize('cacti'))\n",
    "print(lemmatizer.lemmatize('geese'))\n",
    "print(lemmatizer.lemmatize('children'))\n",
    "print(lemmatizer.lemmatize('plays'))\n",
    "print(lemmatizer.lemmatize('rocks'))\n",
    "print(lemmatizer.lemmatize('python'))\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"a\")) #a is for adjective\n",
    "print(lemmatizer.lemmatize('best',pos=\"a\"))\n",
    "print(lemmatizer.lemmatize('run'))\n",
    "print(lemmatizer.lemmatize('run',\"v\")) #v is for verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['This', 'is', 'a', 'sample', 'English', 'sentence']\n",
      "2-gram:  ['This is', 'is a', 'a sample', 'sample English', 'English sentence']\n",
      "3-gram:  ['This is a', 'is a sample', 'a sample English', 'sample English sentence']\n",
      "4-gram:  ['This is a sample', 'is a sample English', 'a sample English sentence']\n",
      "5-gram:  ['This is a sample English', 'is a sample English sentence']\n",
      "6-gram:  ['This is a sample English sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "#Function to extract n-grams from text\n",
    "def get_ngrams(text,n):\n",
    "    n_grams = ngrams(nltk.word_tokenize(text),n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "text = \"This is a sample English sentence\"\n",
    "print(\"1-gram: \",get_ngrams(text,1))\n",
    "print(\"2-gram: \",get_ngrams(text,2))\n",
    "print(\"3-gram: \",get_ngrams(text,3))\n",
    "print(\"4-gram: \",get_ngrams(text,4))\n",
    "print(\"5-gram: \",get_ngrams(text,5))\n",
    "print(\"6-gram: \",get_ngrams(text,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech(PoS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('English', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import chunk\n",
    "tagged_sent = nltk.pos_tag(nltk.word_tokenize(\"This is a sample English sentence\"))\n",
    "print(tagged_sent)\n",
    "tree = chunk.ne_chunk(tagged_sent)\n",
    "tree.draw() #this will draw the sentence tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "#To get help about tags\n",
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Doc_1.txt  Doc_2.txt  Doc_3.txt\n",
      "analytics            1          1          0\n",
      "and                  1          1          1\n",
      "are                  1          1          0\n",
      "books                0          0          1\n",
      "domain               0          1          0\n",
      "equally              1          0          0\n",
      "for                  1          1          0\n",
      "important            1          1          0\n",
      "knowledge            0          1          0\n",
      "like                 0          0          1\n",
      "programming          1          0          0\n",
      "reading              0          0          1\n",
      "skills               2          1          0\n",
      "statistics           1          1          0\n",
      "travelling           0          0          1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#function to create a dictionary with key as filenames and values as text for all the files in a given folder\n",
    "def CorpusFromDir(dir_path):\n",
    "    result = dict(docs=[open(os.path.join(dir_path,f)).read() for f in os.listdir(dir_path)],ColNames=map(lambda x:x,os.listdir(dir_path)))\n",
    "    return result\n",
    "docs = CorpusFromDir(\"C:/Users/Ahmed Khan/Desktop/Data\")\n",
    "#Initialize\n",
    "vectorizer = CountVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(docs.get('docs'))\n",
    "#Create DataFrame\n",
    "df = pd.DataFrame(doc_vec.toarray().transpose(), index = vectorizer.get_feature_names())\n",
    "#Change column headers to be file names\n",
    "df.columns = docs.get('ColNames')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Doc_1.txt  Doc_2.txt  Doc_3.txt\n",
      "analytics     0.276703   0.315269   0.000000\n",
      "and           0.214884   0.244835   0.283217\n",
      "are           0.276703   0.315269   0.000000\n",
      "books         0.000000   0.000000   0.479528\n",
      "domain        0.000000   0.414541   0.000000\n",
      "equally       0.363831   0.000000   0.000000\n",
      "for           0.276703   0.315269   0.000000\n",
      "important     0.276703   0.315269   0.000000\n",
      "knowledge     0.000000   0.414541   0.000000\n",
      "like          0.000000   0.000000   0.479528\n",
      "programming   0.363831   0.000000   0.000000\n",
      "reading       0.000000   0.000000   0.479528\n",
      "skills        0.553405   0.315269   0.000000\n",
      "statistics    0.276703   0.315269   0.000000\n",
      "travelling    0.000000   0.000000   0.479528\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "docs = CorpusFromDir('C:/Users/Ahmed Khan/Desktop/Data')\n",
    "doc_vec = vectorizer.fit_transform(docs.get('docs'))\n",
    "#Create DataFrame\n",
    "df = pd.DataFrame(doc_vec.toarray().transpose(), index=vectorizer.get_feature_names())\n",
    "#Change column headers to be filenames\n",
    "df.columns = docs.get('ColNames')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
